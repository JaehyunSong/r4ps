---
title: "第11回講義資料"
subtitle: "ロジスティック回帰分析"
toc: true
metadata-files: 
  - _material.yml
---

## スライド {.unlisted}

<a href="../../slides/macro/logit.html" class="btn btn-primary btn-sm" target="_blank" role="button"><i class="bi bi-window"></i> 新しいタブで開く</a>

```{=html}
<iframe class="slide-deck" src="../../slides/macro/logit.html" width="100%" style="aspect-ratio: 16 / 9.2;"></iframe>
```

## セットアップ

```{r}
#| include: false
pacman::p_load(tidyverse, gt, summarytools, modelsummary,
               latex2exp, marginaleffects)
# df <- read_csv("materials/Data/Hino_Song.csv")
df <- read_csv("Data/Hino_Song.csv")
```

　本日の実習用データ（LMSからダウンロード可能）と必要なパッケージ（{tidyverse}、{summarytools}、{marginaleffects}）を読み込む。データを読み込み、`df`という名のオブジェクトとして格納する。

```{r}
#| eval: false
#| filename: "Code 01"
library(tidyverse)
library(summarytools)
library(marginaleffects)

df <- read_csv("Data/Hino_Song.csv")

df
```

　各変数の詳細はスライドを参照すること。データ分析を進める前に、まず{summarytools}の`descr()`関数を使って`df`の各変数の記述統計量を確認する。今回のデータの場合、全て連続変数（ダミー変数も連続変数として扱う）であるため、`df`をそのまま`descr()`に渡せば良い。

```{r}
#| filename: "Code 02"
df %>%
  descr(stats = c("mean", "sd", "min", "max", "n.valid"),
        transpose = TRUE, order = "p")
```

　今回は以下の問いに答えるモデルの推定を行う。

> 　有権者の投票参加を規定する要因を調べたい。投票所に足を運ぶには予め投票先を決めておく必要があろう。しかし、数多い選択肢（候補者、政党）の中から自分の望みを実現してくれそうな選択肢を見つけることは簡単な作業ではない。政治に関する知識があれば、比較的簡単に見つかるため、投票参加しやすいと考えられる。一方、そうでない有権者は自分にとっても最適な選択肢を見つけることを諦め、棄権するだろう。これは本当だろうか。

　まず、応答変数は回答者の投票参加の有無（`Voted`）である。こちらは投票すれば1、危険すれば0の値を取るダミー変数であり、応答変数として使う場合は**二値変数**、または**バイナリー変数**とも呼ばれる。続いて、主な説明変数は回答者の主観的な政治知識（`Knowledge`）である。また、主な説明変数以外の説明変数（=統制変数）として性別（`Female`）、年齢（`Age`）、学歴（`Educ`）、イデオロギー（`Ideology`）を投入する。このモデルを可視化すると以下のようになる。

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 3
tibble(x = c(1, 1, 1, 1, 1),
       xend = c(2, 2, 2, 2, 2),
       y = c(5, 4, 3, 2, 1),
       yend = c(5, 5, 5, 5, 5)) %>%
  ggplot() +
  geom_segment(aes(x = x, xend = xend, y = y, yend = yend),
               arrow = arrow(type = "closed", length = unit(0.1, "inches"))) +
  geom_label(aes(x = c(1, 1, 1, 1, 1), y = c(5, 4, 3, 2, 1), 
                 label = c("主観的な政治知識",
                           "女性ダミー",
                           "年齢",
                           "学歴",
                           "イデオロギー")), hjust = 1, size = 5) +
  geom_label(aes(x = 2, y = 5, label = "投票参加"), hjust = 0, size = 5) +
  coord_cartesian(xlim = c(0.5, 2.2)) +
  theme_void()
```

## 線形確率モデル

　まずは、これまで紹介してきた線形回帰分析を使って、モデルを推定してみよう。推定する回帰式は以下の通りである。

$$
\widehat{\mbox{Voted}} = \alpha + \beta_1 \mbox{Knowledge} + \beta_2 \mbox{Female} + \beta_3 \mbox{Age} + \beta_4 \mbox{Educ} + \beta_5 \mbox{Ideology}
$$

```{r}
#| filename: "Code 03"
lm_fit <- lm(Voted ~ Knowledge + Female + Age + Educ + Ideology, data = df)

summary(lm_fit)
```

　推定の結果、政治知識（`Knowledge`）の係数は`r round(coef(lm_fit)[2], 3)`が得られた。これは「政治知識が1単位上がると、投票参加の確率は約`r round(coef(lm_fit)[2] * 100, 1)`%ポイント上がる」ことを意味する（「%」でなく、「%ポイント」であることに注意）。$p$値も非常に小さく（$p < 0.05$）、統計的に有意な関係であると言えよう。

　このようにバイナリー変数を応答変数として使う線形回帰モデルを**線形確率モデル**（linear probability model; LPM）と呼ぶ。これは非常に直感的な方法であるものの、一つ大きな問題がある。たとえば、主観的政治知識（`Knowledge` = 5）が5、男性（`Female` = 0）、86歳（`Age` = 86）、教育水準が4（`Educ` = 4）、イデオロギーが10（`Ideology ` = 10）の回答者がいる場合、投票に参加する確率の予測値を計算してみよう。

```{r}
#| filename: "Code 04"
-0.1093644 + 0.1186971 * 5 - 0.0549171 * 0 + 0.0046902 * 86 + 0.0357970 * 4 + 0.0040744 * 10
```

　主観的政治知識が5、男、86歳、教育水準が4、イデオロギーが10の回答者が投票に参加する確率は約107.1%である。確率は0〜100%の値を取るはずなのに、100%を超えてしまう。 他にも線形確率モデルは、線形回帰モデルの重要な仮定である分散の不均一性の満たさないなど、いくつかの問題がある。ここで登場するのがロジスティック回帰分析だ。

## ロジスティック回帰分析

### ロジスティック関数

　ロジスティック回帰分析を理解するためには、その背後にはるロジスティック関数に関する知識が必要である。ロジスティック関数は以下のような関数である。

$$
\text{logistic}(x) = \frac{1}{1 + e^{-x}}
$$

　式内の$e$はネイピア数であり、2.71828184590...の無理数である。$x$は$-\infty \sim \infty$の値を取り得る。たとえば、$x$が$-\infty$の場合、$e^{-x}$は$e^{-(-\infty)} = \infty$、$x$が$\infty$の場合、$e^{-x}$は$e^{-\infty} = 0$となる。つまり、$e^{-x}$は$0 \sim \infty$の値を取る。この$e^{-x}$が0の場合、$\text{logistic}(x)$は1、$e^{-x}$が$\infty$の場合、$\text{logistic}(x)$は0を取る。つまり、$\text{logistic}(x)$は0以上、1以下の取ることが分かる。確率もまた0以上1以下であるため、この関数が何らかの役割を果たすこととなる。以下の図は$x$の値ごとのロジスティック関数の折れ線グラフである。

```{r}
#| echo: false
#| fig-width: 8
#| figh-height: 4
tibble(x = seq(-10, 10, 0.1)) %>%
  mutate(y = 1 / (1 + exp(-x))) %>%
  ggplot() +
  geom_line(aes(x = x, y = y), linewidth = 2) +
  labs(x = "x", y = "logistic(x)") +
  theme_bw()
```

　$x$の値が大きければ大きいほど、logistic($x$)の値は1へ近づくことが分かる。この$x$を$y^*$と表記した場合、ロジスティック回帰分析は

$$
\begin{align}
\mbox{Pr}(\mbox{Voted} = 1) & = \frac{1}{1 + e^{-y^*}} \\
y^* & = \alpha + \beta_1 \mbox{Knowledge} + \beta_2 \mbox{Female} + \beta_3 \mbox{Age} + \beta_4 \mbox{Educ} + \beta_5 \mbox{Ideology}
\end{align}
$$

における$\alpha$、$\beta_1$、...を推定することである。ここでの$y^*$は**線形予測子**（linear predictor）と呼ばれる。

### 実装

　ロジスティックの実装は`glm()`関数を使用し、使い方は`lm()`関数とほぼ同様である。

```{r}
#| eval: false
glm(応答変数 ~ 説明変数, data = データ名, family = binomial("logit"))
```

　`lm()`との違いは`family`引数が必要という点だ。ロジスティック回帰分析の場合は`family = binomial("logit")`、本講義では取り上げないものの同目的の分析手法であるプロビット回帰分析の場合は`family = binomial("probit")`を使用する。それでは以下のモデルを推定し、`fit1`に格納してみよう。

$$
\mbox{Pr}(\mbox{Voted} = 1) = \frac{1}{1 + e^{-(\alpha + \beta_1 \mbox{Knowledge} + \beta_2 \mbox{Female} + \beta_3 \mbox{Age} + \beta_4 \mbox{Educ} + \beta_5 \mbox{Ideology})}}
$$

```{r}
#| filename: "Code 05"
fit1 <- glm(Voted ~ Knowledge + Female + Age + Educ + Ideology, 
            data = df, family = binomial("logit"))

summary(fit1)
```

:::{.callout-note}
## ロジスティック回帰分析の決定係数?

ロジスティック回帰分析の場合、決定係数（$R^2$）は表示されない。類似した概念として「疑似決定係数（Pseudo$R^2$）」があるが、あまり使われない。どうしても疑似決定係数を出したい場合は{DescTools}パッケージの`PseduoR2()`関数を使う。

```{r}
#| filename: "Code 06"
# 予め{DescTools}をインストールしておくこと (コンソール上でinstall.packages("DescTools"))
DescTools::PseudoR2(fit1) # McFaddenの疑似決定係数
```
:::

　それでは結果を解釈してみよう。線形確率モデルと同様、政治知識の$p$値は0.001未満であり、「主観的な政治知識と投票参加の間には統計的に有意な関係がある」ことが分かる。また、係数の符号（今回は+）を考えると、「主観的な政治知識が高くなると、投票に参加する確率も上がる」とも言えよう。しかし、政治知識が1単位上がると、投票参加の確率は**具体的にどれくらい上がるか**。線形確率モデルの場合は係数をそのまま解釈するだけで、具体的な上がりの度合いが分かったが、ロジスティック回帰分析では違う。

　たとえば、政治知識の係数は約0.593であるが、「主観的な政治知識が1上がると、投票参加の確率が0.593%p上がる」ということは完全な間違いである。これをどう解釈すべきだろうか。

　たおてば、主観的政治知識が3（`Knowledge` = 3）、女性（`Female` = 1）、20歳（`Age` = 20）、学歴が大卒（`Educ` = 4）、イデオロギーが中道（`Ideology` = 5）の場合の投票参加の予測確率を計算してみよう。まず、`coef()`関数で係数のみを抽出してみよう。

```{r}
#| filename: "Code 07"
fit1_coef <- coef(fit1) # coef()関数で係数のみを抽出する
fit1_coef
```

　続いて、切片には1を、その他の係数には各変数の具体的な値をかけて合計を求める。これが線形予測子だ。

```{r}
#| filename: "Code 08"
fit1_coef[1] * 1 + fit1_coef[2] * 3 + fit1_coef[3] * 1 + fit1_coef[4] * 20 + fit1_coef[5] * 4 + fit1_coef[6] * 5
```

　同じ長さのベクトルが2つあれば、同じ位置同士の要素が計算されることを考えると、以下のように線形予測子を計算することもできる。

```{r}
#| filename: "Code 09"
sum(coef(fit1) * c(1, 3, 1, 20, 4, 5))
```

　あとは線形予測子の値をロジスティック関数に代入するだけだ。

```{r}
#| filename: "Code 10"
1 / (1 + exp(-(-0.3602094))) # 予測確率の計算
```

　結果は`r round(1 / (1 + exp(-(-0.3602094))), 3)`であり、これは投票参加の予測確率は約`r round(1 / (1 + exp(-(-0.3602094))) * 100, 1)`%であるこを意味する（以下の図を参照）。

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 4.3
tibble(x = seq(-10, 10, 0.1)) %>%
  mutate(y = 1 / (1 + exp(-x))) %>%
  ggplot() +
  geom_line(aes(x = x, y = y), linewidth = 2) +
  geom_vline(xintercept = -0.3602094, color = "red") +
  geom_hline(yintercept = 0.4109089, color = "red") +
  labs(x = "x", y = expression(frac(1, (1+e^-x)))) +
  scale_y_continuous(breaks = c(0, 0.25, 0.41, 0.50, 0.75, 1.00),
                     labels = c(0, 0.25, 0.41, 0.50, 0.75, 1.00)) +
  scale_x_continuous(breaks = c(-10, -5, -0.36, 0, 5, 10),
                     labels = c(-10, -5, -0.36, "", 5, 10)) +
  theme_bw() +
  theme(panel.grid = element_blank())
```

　このような作業をより簡単にしてくれるのが、前回の講義で登場した{marginaleffects}パッケージである。このパッケージが提供する`predictions()`関数の利用すれば予測確率が簡単に計算できる。使い方は以下の通りである。

```{r}
#| eval: false
predictions(回帰分析オブジェクト名, 
            newdata = datagrid(説明変数名1 = 値,
                               説明変数名2 = 値,
                               ...))
```
   
　たとえば、先程の例を`predictions()`関数で計算してみよう。

```{r}
#| filename: "Code 11"
predictions(fit1, newdata = datagrid(Knowledge = 3, 
                                     Female    = 1,
                                     Age       = 20, 
                                     Educ      = 4, 
                                     Ideology  = 5))
```

　`datagrid()`内で変数を特定の値で指定しない場合、その変数は自動的に**平均値**が代入される。たとえば、`Knowledge`が1〜5の場合の投票参加確率の予測値を計算してみよう。その他の変数はすべて平均値に固定する。計算結果は`fit1_pred`に格納する。

```{r}
#| filename: "Code 12"
fit1_pred <- predictions(fit1, newdata = datagrid(Knowledge = 1:5))

fit1_pred
```

　`Knowledge`の値に応じた投票参加の予測確率（0以上、1以下）が`Estimate`列に出力され、95%信頼区間の下限と上限は`2.5%`、`97.5%`列に表示される。ただし、**こちらで表示された列名は本当の列名ではない**。つまり、この`fit1_pred`を用いて作図をする際、`Estimate`や`2.5%`という名前でマッピングすることはできない。今出力された列名はあくまでも読みやすさを重視したものであり、実際の列名は異なる。他にも実際にはあるものの出力されていない列も存在する。すべての列を、本当の列名と共に出力するなら、`print()`関数を使用し、`style = "data.frame"`を追加しよう。

```{r}
#| filename: "Code 13"
print(fit1_pred, style = "data.frame")
```

　`Estimate`の本当の列名は`estimate`、`2.5%`と`97.5%`の本当の列名は`conf.low`、`conf.high`であることが分かる。今回はここまで知らなくても良いかも知れないが、第13回（推定結果の可視化）では、この点を理解しないと絶対に作図できないため注意しておこう。

　話を戻そう。これまでの結果を見れば、`Knowledge`が`Voted`に与える影響は一定ではないことがわかる。たとえば、`Knowledge`の値が1の場合、投票に参加する確率は32.94%である。ここで`Knowledge`の値が2の場合、投票に参加する確率は47.05%であり、14.11%**p**増加したことが分かる（14.11「%」でなく、14.11「%p」であることに注意しよう。%pは「パーセントポイント」と読む）。それでは`Knowledge`の値が3の場合はどうだろうか。この場合の投票参加の確率は61.65%であり、先より14.60%**p**増加した。また、`Knowledge`の値が4の場合の予測確率は74.42%（12.77%**p**増加）、5の場合のそれはは84.03%で、9.61%**p**増加したことが分かる。このようにロジスティック関数（から得られた曲線）の形が非線形であることを考えると、ある意味、当たり前のことであろう。

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 4.5
fit1_temp <- predictions(fit1, 
                         newdata = datagrid(Knowledge = seq(-10, 13, 0.1)))
fit1_pred %>%
  ggplot() +
  geom_line(data = fit1_temp, aes(x = Knowledge, y = estimate)) +
  geom_point(aes(x = Knowledge, y = estimate), size = 3) +
  scale_x_continuous(breaks = 1:5, labels = 1:5) +
  labs(x = "主観的な政治知識", 
       y = "投票に参加する確率の予測値") +
  theme_bw(base_size = 14) +
  theme(panel.grid.minor.x = element_blank())
```


　したがって、ロジスティック回帰分析を行う場合、「Xが1単位上がるとYはZZ上がる/下がる」という表現はあまり使わず、文章では「正（負）の関係がある」、「統計的に有意な関係は見られない」程度とし、予測確率をグラフとして示すのが一般的である。たとえば、以上の結果を可視化したものが以下の図である。可視化については次回の講義で解説する。

```{r}
#| filename: "Code 14"
#| code-fold: true
#| fig-width: 8
#| fig-height: 4
fit1_pred %>%
  ggplot() +
  geom_pointrange(aes(x = Knowledge, y = estimate, 
                      ymin = conf.low, ymax = conf.high)) +
  labs(x = "主観的な政治知識", 
       y = "投票に参加する確率の予測値") +
  theme_bw(base_size = 14)
```

　`Age`のように説明変数の中でも取り得る値が多い場合は以下のような図になってしまい、人によっては少し気持ち悪い図になってしまう。

```{r}
#| filename: "Code 14"
#| code-fold: true
#| fig-width: 8
#| fig-height: 4
predictions(fit1, newdata = datagrid(Age = 18:86)) %>%
  ggplot() +
  geom_pointrange(aes(x = Age, y = estimate, 
                      ymin = conf.low, ymax = conf.high)) +
  labs(x = "年齢 (歳)", y = "投票に参加する確率の予測値") +
  scale_x_continuous(breaks = c(20, 30, 40, 50, 60, 70, 80), 
                     labels = c(20, 30, 40, 50, 60, 70, 80)) +
  theme_bw(base_size = 14)
```

　したがって、取り得る値が多い連続変数を横軸にする場合は、折れ線グラフ (`geom_line()`)とリボン (`geom_ribbon()`)を併用することが一般的だ。

```{r}
#| filename: "Code 14"
#| code-fold: true
#| fig-width: 8
#| fig-height: 4
fit1_pred2 <- predictions(fit1, newdata = datagrid(Age = 18:86))

fit1_pred2 %>%
  ggplot() +
  geom_ribbon(aes(x = Age, ymin = conf.low, ymax = conf.high), 
              fill = "gray80") + 
  geom_line(aes(x = Age, y = estimate)) +
  labs(x = "年齢 (歳)", y = "投票に参加する確率の予測値") +
  scale_x_continuous(breaks = c(20, 30, 40, 50, 60, 70, 80), labels = c(20, 30, 40, 50, 60, 70, 80)) +
  theme_bw(base_size = 14)
```
